{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "E01"
      ],
      "metadata": {
        "id": "Y4ZSWykuzKto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When all weights and biases are initialized to zero, the network does not remain completely stuck because the output bias receives a non-zero gradient from the softmaxâ€“cross-entropy loss. As a result, the output bias learns global character frequencies, effectively turning the model into a unigram language model. However, all weight matrices and earlier biases receive zero gradients due to symmetry: the hidden activations are zero and identical for all neurons, so no symmetry breaking occurs. Consequently, only the output bias is trained, the network ignores context, and the final performance is poor."
      ],
      "metadata": {
        "id": "qywZkUUpzJXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E02"
      ],
      "metadata": {
        "id": "CRJMx_hNzMG2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Goh2BHmkyaEM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# dimensions\n",
        "nin = 10\n",
        "nh1 = 64\n",
        "nh2 = 64\n",
        "nout = 5\n",
        "eps = 1e-5\n",
        "\n",
        "# dummy data\n",
        "X = torch.randn(512, nin)\n",
        "Y = torch.randint(0, nout, (512,))\n",
        "\n",
        "# layer 1\n",
        "W1 = torch.randn(nin, nh1) * 0.1\n",
        "b1 = torch.zeros(nh1)\n",
        "bn1_gamma = torch.ones(nh1)\n",
        "bn1_beta = torch.zeros(nh1)\n",
        "bn1_running_mean = torch.zeros(nh1)\n",
        "bn1_running_var = torch.ones(nh1)\n",
        "\n",
        "# layer 2\n",
        "W2 = torch.randn(nh1, nh2) * 0.1\n",
        "b2 = torch.zeros(nh2)\n",
        "bn2_gamma = torch.ones(nh2)\n",
        "bn2_beta = torch.zeros(nh2)\n",
        "bn2_running_mean = torch.zeros(nh2)\n",
        "bn2_running_var = torch.ones(nh2)\n",
        "\n",
        "# output layer (no BN)\n",
        "W3 = torch.randn(nh2, nout) * 0.1\n",
        "b3 = torch.zeros(nout)\n",
        "\n",
        "params = [\n",
        "    W1, b1, bn1_gamma, bn1_beta,\n",
        "    W2, b2, bn2_gamma, bn2_beta,\n",
        "    W3, b3\n",
        "]\n",
        "for p in params:\n",
        "    p.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(2000):\n",
        "\n",
        "    # forward\n",
        "    h1 = X @ W1 + b1\n",
        "    mu1 = h1.mean(0)\n",
        "    var1 = h1.var(0, unbiased=False)\n",
        "\n",
        "    bn1_running_mean = 0.9 * bn1_running_mean + 0.1 * mu1\n",
        "    bn1_running_var  = 0.9 * bn1_running_var  + 0.1 * var1\n",
        "\n",
        "    h1n = (h1 - mu1) / torch.sqrt(var1 + eps)\n",
        "    h1 = torch.tanh(bn1_gamma * h1n + bn1_beta)\n",
        "\n",
        "    h2 = h1 @ W2 + b2\n",
        "    mu2 = h2.mean(0)\n",
        "    var2 = h2.var(0, unbiased=False)\n",
        "\n",
        "    bn2_running_mean = 0.9 * bn2_running_mean + 0.1 * mu2\n",
        "    bn2_running_var  = 0.9 * bn2_running_var  + 0.1 * var2\n",
        "\n",
        "    h2n = (h2 - mu2) / torch.sqrt(var2 + eps)\n",
        "    h2 = torch.tanh(bn2_gamma * h2n + bn2_beta)\n",
        "\n",
        "    logits = h2 @ W3 + b3\n",
        "    loss = F.cross_entropy(logits, Y)\n",
        "\n",
        "    # backward\n",
        "    for p in params:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    for p in params:\n",
        "        p.data += -0.1 * p.grad"
      ],
      "metadata": {
        "id": "bYKeb1u8ymOl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1_fused = (bn1_gamma / torch.sqrt(bn1_running_var + eps)) * W1\n",
        "b1_fused = (\n",
        "    bn1_gamma / torch.sqrt(bn1_running_var + eps)\n",
        ") * (b1 - bn1_running_mean) + bn1_beta\n"
      ],
      "metadata": {
        "id": "JWFwQ6ANyrzC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W2_fused = (bn2_gamma / torch.sqrt(bn2_running_var + eps)) * W2\n",
        "b2_fused = (\n",
        "    bn2_gamma / torch.sqrt(bn2_running_var + eps)\n",
        ") * (b2 - bn2_running_mean) + bn2_beta\n"
      ],
      "metadata": {
        "id": "8dL0SmTYyu7m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def forward_with_bn(X):\n",
        "    h1 = X @ W1 + b1\n",
        "    h1 = (h1 - bn1_running_mean) / torch.sqrt(bn1_running_var + eps)\n",
        "    h1 = torch.tanh(bn1_gamma * h1 + bn1_beta)\n",
        "\n",
        "    h2 = h1 @ W2 + b2\n",
        "    h2 = (h2 - bn2_running_mean) / torch.sqrt(bn2_running_var + eps)\n",
        "    h2 = torch.tanh(bn2_gamma * h2 + bn2_beta)\n",
        "\n",
        "    return h2 @ W3 + b3\n"
      ],
      "metadata": {
        "id": "Z6xjPkzlyxQK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def forward_fused(X):\n",
        "    h1 = torch.tanh(X @ W1_fused + b1_fused)\n",
        "    h2 = torch.tanh(h1 @ W2_fused + b2_fused)\n",
        "    return h2 @ W3 + b3\n"
      ],
      "metadata": {
        "id": "-y5RYNonyzwb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out1 = forward_with_bn(X)\n",
        "out2 = forward_fused(X)\n",
        "\n",
        "print(\"max absolute difference:\", (out1 - out2).abs().max().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKnz9HG-y4fy",
        "outputId": "6187e9d9-b030-4c60-f9a4-af765b656a1d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max absolute difference: 7.62939453125e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BatchNorm can be folded into the preceding Linear layer at inference time by absorbing its scale and shift parameters into the weights and biases. After training a 3-layer MLP with BatchNorm, we computed new weights and biases using the running mean and variance together with gamma and beta. Removing BatchNorm and using the fused Linear layers produced identical outputs during inference (up to numerical precision), confirming that BatchNorm can be safely erased at test time."
      ],
      "metadata": {
        "id": "98uEsX_Ry_wv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-G5cleXJy6cK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}