{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6818df5d",
   "metadata": {},
   "source": [
    "# Week 4 â€“ WaveNet-style Hierarchical MLP\n",
    "\n",
    "This notebook builds a hierarchical (tree-structured) WaveNet-style MLP for character-level name modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2888e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c1df6",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84adc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load names\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "# build vocab\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "block_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fde570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0]*block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xva, Yva = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f78ff",
   "metadata": {},
   "source": [
    "## WaveNet-style building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758553a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WaveBlock(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_in, n_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B*T, C)\n",
    "        x = self.net(x)\n",
    "        x = x.view(B, T, -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9c07c",
   "metadata": {},
   "source": [
    "## Hierarchical WaveNet MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccd76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WaveNetMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        self.l1 = WaveBlock(2*n_embd, n_embd)\n",
    "        self.l2 = WaveBlock(2*n_embd, n_embd)\n",
    "        self.l3 = WaveBlock(2*n_embd, n_embd)\n",
    "\n",
    "        self.out = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        x = self.embed(idx)      # (B, 8, C)\n",
    "\n",
    "        x = x.view(x.shape[0], 4, -1)\n",
    "        x = self.l1(x)\n",
    "\n",
    "        x = x.view(x.shape[0], 2, -1)\n",
    "        x = self.l2(x)\n",
    "\n",
    "        x = x.view(x.shape[0], 1, -1)\n",
    "        x = self.l3(x)\n",
    "\n",
    "        logits = self.out(x.squeeze(1))\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d833c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = WaveNetMLP(vocab_size, n_embd=24)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "lossi = []\n",
    "steps = 20000\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(steps):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    xb, yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    logits = model(xb)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "    if i % 2000 == 0:\n",
    "        print(i, loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed90f6f9",
   "metadata": {},
   "source": [
    "## Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0892d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(lossi)\n",
    "plt.ylim(0, 3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca653a",
   "metadata": {},
   "source": [
    "## Validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e78c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(X, Y):\n",
    "    logits = model(X)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    return loss.item()\n",
    "\n",
    "print(\"Train:\", split_loss(Xtr, Ytr))\n",
    "print(\"Val  :\", split_loss(Xva, Yva))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c29911",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def sample(n=20):\n",
    "    for _ in range(n):\n",
    "        context = [0]*block_size\n",
    "        out = ''\n",
    "        while True:\n",
    "            x = torch.tensor([context])\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, 1).item()\n",
    "            context = context[1:] + [ix]\n",
    "            if ix == 0:\n",
    "                break\n",
    "            out += itos[ix]\n",
    "        print(out)\n",
    "\n",
    "sample(20)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}